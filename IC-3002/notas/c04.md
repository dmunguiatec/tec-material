---
title: "Capítulo 4. Análisis de casos"
author: "Diego Munguía Molina ^[Esta obra está bajo una Licencia Creative Commons Atribución 4.0 Internacional.]"
date: "Marzo, 2019"
institute: "Ingeniería en Computación, TEC"
geometry: margin=1in
header-includes:
    - \usepackage{setspace}
    - \providecommand{\subtitle}[1]{\usepackage{titling} \posttitle{\par\large#1\end{center}}}
    - \usepackage[spanish]{babel}
    - \usepackage[linesnumbered,ruled,vlined,spanish,onelanguage]{algorithm2e}
    - \usepackage{amssymb}
    - \SetKwInput{KwInput}{Entradas}
    - \SetKwInput{KwOutput}{Salidas}
    - \newcommand{\twodots}{\mathrel{{.}\,{.}}\nobreak}
    - \newcommand{\assign}{\leftarrow}
    - \DeclareMathOperator{\inter}{inter}
output:
  pdf_document:
    latex_engine: xelatex
---

Retomando el problema de ordenamiento, analicemos ahora la complejidad temporal del algoritmo de ordenamiento por inserción (_Insertion Sort_).

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Una secuencia de $n$ enteros $A$ tal que $n \ge 1$ y $\forall i \in [0 \twodots (n-1)[$ se cumple que $a_i \le a_{i+1} \lor a_i \ge a_{i+1}$}
    \KwOutput{Una permutación de $A$, $A^\prime = [a_1, a_2, \dots, a_n]$ tal que $a_1 \le a_2 \le \dots \le a_n$}
  
    \BlankLine
    \caption{Ordenamiento por inserción}
    \SetAlgoVlined
    
    \For{$i \in [0 \twodots n[$} {
        $j \assign i$ \;
        \While{$j > 0 \land A[j] < A[j-1]$} {
            $tmp \assign A[j]$ \;
            $A[j] \assign A[j-1]$ \;
            $A[j-1] \assign tmp$ \;

            $j \assign j-1$ \;
        }
    }

\end{algorithm}

En este algoritmo observamos que la fuente de complejidad temporal surge de los ciclos anidados en las líneas 1 y 3.

Debido a la línea 2, donde definimos el índice $j$ del ciclo interior con base en el índice $i$ del ciclo exterior, establecemos que ambos ciclos son dependientes entre si y por tanto deben ser analizados en conjunto.

Observamos que el ciclo exterior hace un recorrido completo de $n$ iteraciones, desde $0$ hasta $(n-1)$. Sin embargo, cuando observamos el ciclo interior podemos notar que su condición de parada depende del contenido de la secuencia $A$, pues el segundo término de la conjunción lógica establece que $A[j] < A[j-1]$ se debe cumplir para poder seguir iterando.

Este hecho complica el análisis, pues no podemos determinar cuántas veces repetirá el ciclo interior sin conocer de antemano cuál será el contenido de la secuencia $A$ que será ordenada.

Cuando la complejidad de un algoritmo depende de los datos con los que será ejecutado tenemos que hacer un análisis de casos. Este análisis requiere establecer casos, entendidos como las posibles distintas configuraciones de los datos de entrada de un mismo tamaño que representen correspondiente la peor y mejor expectativas posibles sobre la complejidad en cuestión. Además del mejor y peor caso posible, también podemos incluir el caso promedio en nuestro análisis.

## Peor caso ##

Definición
 ~ El peor caso para un algoritmo está determinado por la configuración de entradas      que produzca el máximo consumo del recurso analizado.

Volviendo a nuestro ejemplo de ordenamiento por inserción, el peor caso está determinado por la permutación de $A$ de tamaño $n$ que obligue al algoritmo a ejecutar el número máximo de iteraciones del ciclo interior posibles. Esta permutación puede ser cualquier secuencia ordenada descendentemente.

Tomemos por ejemplo:

$$
A = [9,5,3,1,0]
$$

Cuando $j=1$ tendremos que el $5$ será menor que el $9$ y por tanto deberán ser intercambiados.

$$
A = [5,9,3,1,0]
$$

En la siguiente iteración con $j=2$ el $3$ será menor que el $5$ y $9$, y por tanto serán necesarios dos intercambios.

$$
A = [3,5,9,1,0]
$$

Claramente con $j=3$ y $j=4$ se repite el patrón pues el $1$ será menor que todos los números a su izquierda, y lo mismo sucederá con el $0$.

Cuando la secuencia está ordenada descendentemente, el término $A[j] < A[j-1]$ en la condición de parada del ciclo interior siempre será verdadero. De esta forma el ciclo pasa a ser dirigido por el primer término de la conjunción $j > 0$.

Esto hace que el ciclo interior repita $i$ veces desde $j=i$ hasta $j=1$.

Podemos entonces formular la función de complejidad temporal $T_{peor}$ para el peor caso

$$
\begin{aligned}
T_{peor}(n) &= \sum_{i=1}^{n} \sum_{j=1}^{i} 1 \\
 &= \sum_{i=1}^{n} i \\
 &= \frac{n(n+1)}{2} \\
 & \therefore T_{peor}(n) = \mathcal{O}(n^2)
\end{aligned}
$$

## Mejor caso ##

Definición
 ~ El mejor caso para un algoritmo está determinado por la configuración de entradas     que produzca el mínimo consumo del recurso analizado.

En el caso del algoritmo de ordenamiento por inserción, el mejor caso está dado por la permutación de $A$ de tamaño $n$ que ya se encuentra ordenada ascendentemente. Por ejemplo

$$
A = [0,1,3,5,9]
$$

En este caso, para cualquier $j \in [1 \twodots 4]$ los números a la izquierda de $j$ siempre serán menores que $A[j]$. Provocando de esta forma que el término $A[j] < A[j-1]$ en la condición de parada del ciclo interior siempre sea falso; por lo tanto, para cualquier $i$, el ciclo interior nunca se ejecutará.

Podemos formular entonces la función de complejidad temporal para el mejor caso $T_{mejor}$ como

$$
\begin{aligned}
T_{mejor}(n) &= \sum_{i=1}^{n} 1 \\
 &= n \\
 &\therefore T_{mejor}(n) = \mathcal{O}(n)
\end{aligned}
$$

## Caso promedio ##

Definición
 ~ El caso promedio para un algoritmo está determinado por el consumo promedio del       recurso analizado con base en todas las posibles configuraciones de entrada.

Observar todas las posibles configuraciones de las entradas es una tarea en muchos casos inmanejable y en otros casos simplemente imposible. Por esta razón, podemos utilizar teoría de probabilidades para estimar la complejidad del caso promedio.

Para el caso promedio del ordenamiento por inserción, primero debemos tomar en cuenta que existen $n!$ posibles permutaciones para una secuencia $A$ de tamaño $n$. La estimación parte del supuesto de que cada una de estas permutaciones es equiprobable.

Supongamos que $A$ contiene los números naturales en el rango $[1 \twodots n]$ en un orden no determinado, es decir $A$ es una permutación de $\{1,2,\dots,n\}$.

Sea $(p, q)$ un par tal que $p, q \in A$, $1 \le p < q \le n$. Llamaremos a $(p, q)$ un _intercambio_ en $A$ si se cumple que el índice de $q$ es menor que el índice de $p$ en $A$, es decir $q$ aparece antes que $p$ en la secuencia $A$.

Por ejemplo para

$$
A = [2,5,3,4,1]
$$

identificamos los siguientes intercambios

$$
\{(1, 2), (3, 5), (4, 5), (1, 5), (1, 3), (1, 4)\}
$$

Definimos ahora la variable aleatoria $I_{p, q}$ de la siguiente manera

\begin{equation}
I_{p, q} = \begin{cases}
    1, & \text{si $(p, q)$ es un intercambio en $A$}.\\
    0, & \text{si $(p, q)$ no es un intercambio en $A$}.
  \end{cases}
\end{equation}

Con base en esta variable, definimos ahora la función $\inter(p)$

\begin{equation}
    \inter(p) = \sum_{q=p+1}^{n} I_{p, q}
\end{equation}

como el número total de intercambios de la forma $(p, q)$ en la permutación $A$ para un $p$ dado.

Tomemos ahora un $p = A[k]$ cualquiera. ¿Cuántos intercambios se realizarán cuando se compare con el resto de elementos a su izquierda?

Si $k=0$ entonces no se realiza ningún intercambio puesto que estamos al principio de la secuencia y ya no hay más elementos a su izquierda.

Cuando $k > 0$ si hay intercambios posibles a la izquierda. Sabemos además, precisamente por la definición de intercambio, que todos los $q$ que corresponden a intercambios con $p$ están a la izquierda de $p$.

Por lo tanto, la respuesta es que se realizarán $\inter(p)$ intercambios cuando se compare nuestro $p$ arbitrario con todos los elementos a su izquierda.

Podemos definir tentativamente el número promedio de repeticiones para el algoritmo de ordenamiento por inserción como

$$
\begin{aligned}
T(n) &= \sum_{p=1}^{n} \inter(p) \\
 &= \sum_{p=1}^{n} \sum_{q=p+1}^{n} I_{p, q}
\end{aligned}
$$

Sin embargo este postulado no está completo, aún nos queda pendiente determinar cuándo $I_{p, q}$ será $1$ y cuándo será $0$. Con este fin, debemos recordar que en una permutación aleatoria es igualmente probable tanto que $q$ aparezca antes que $p$ como que $q$ aparezca después de $p$. De manera más formal,

$$
\forall p, q, P(I_{p, q} = 1) = P(I_{p, q} = 0) = \frac{1}{2}
$$

Puesto que nuestra función $T_{prom}(n)$ está compuesta por una sumatoria de variables aleatorias, podemos entonces definirla en términos de su expectativa (ver Apéndice II).

$$
\begin{aligned}
T_{prom}(n) = E\left(\sum_{p=1}^{n} \sum_{q=p+1}^{n} I_{p, q}\right)
\end{aligned}
$$

Puesto que $E(X + Y) = E(X) + E(Y)$

$$
\begin{aligned}
T_{prom}(n) = \sum_{p=1}^{n} \sum_{q=p+1}^{n} E\left(I_{p, q}\right) \\
\end{aligned}
$$

La expectativa para la variable aleatoria $I_{p, q}$ es

$$
E(I_{p, q}) = 1 \cdot P(I_{p, q} = 1) + 0 \cdot P(I_{p, q} = 0) = \frac{1}{2}
$$

Entonces reemplazamos en $T_{prom}(n)$ y simplificamos la expresión

$$
\begin{aligned}
T_{prom}(n) &= \sum_{p=1}^{n} \sum_{q=p+1}^{n} \frac{1}{2} \\
 &= \sum_{p=1}^{n} \frac{1}{2} \sum_{q=p+1}^{n} 1 \\
 &= \sum_{p=1}^{n} \frac{1}{2} \left( \sum_{q=1}^{n} 1 - \sum_{q=1}^{p} 1 \right) \\
 &= \sum_{p=1}^{n} \frac{1}{2} \left( n - p \right) \\
 &= \frac{1}{2} \sum_{p=1}^{n} \left( n - p \right) \\
 &= \frac{1}{2} \left(\sum_{p=1}^{n} n - \sum_{p=1}^{n} p \right) \\
 &= \frac{1}{2} \left(n \cdot \sum_{p=1}^{n} 1 - \sum_{p=1}^{n} p \right) \\
 &= \frac{1}{2} \left(n^2 - \sum_{p=1}^{n} p \right) \\
 &= \frac{1}{2} \left(n^2 - \frac{n(n+1)}{2} \right) \\
 &= \frac{1}{2} \left(n^2 - \frac{n^2+n}{2} \right) \\
 &= \frac{1}{2} \left(\frac{n^2 - n}{2} \right) \\
 &= \frac{n^2 - n}{4} \\
 & \therefore T_{prom}(n) = \mathcal{O}(n^2)
\end{aligned}
$$

## Síntesis final ##

Una vez completos los análisis individuales para cada caso --mejor, peor y promedio--, podemos realizar una síntesis comparativa.

Determinamos que la complejidad temporal del ordenamiento por inserción en el mejor de los casos es lineal $\mathcal{O}(n)$, lo cual es muy eficiente, principalmente en comparación con el ordenamiento por selección o por burbuja, ambos con un único caso con complejidad cuadrática.

Sin embargo, debemos notar que el mejor caso implica recibir la secuencia ya ordenada. De todas las posibles permutaciones de $A$, solo hay una posible permutación que está ordenada ascendentemente, por lo tanto la probabilidad para el mejor caso es $P(mejor) = \frac{1}{n!}$, una probabilidad que se acercará cada vez más a cero conforme crezca el tamaño de la entrada.

Adicionalmente se podría cuestionar el valor o sentido práctico del mejor caso, pues precisamente la tarea del algoritmo es ordenar la secuencia. No obstante, el mejor caso nos sugiere que el algoritmo por inserción _tendrá un buen rendimiento cuando se enfrente a secuencias parcialmente ordenadas_.

El peor caso también tiene una baja probabilidad de ocurrir $P(peor) = \frac{1}{n!}$, pues sólo hay una posible permutación de la secuencia que esté ordenada descendentemente. De manera similar al mejor caso, podemos esperar que el algoritmo en general tenga un mal rendimiento con secuencias parcialmente ordenadas de manera descendente.

Por otro lado, el peor caso y el caso promedio comparten complejidad temporal cuadrática. Podemos esperar que el algoritmo tenga un bajo rendimiento la mayoría de los casos. Pese a esto, también observamos que $T_{peor} \approx \frac{n^2}{2}$ y $T_{prom} \approx \frac{n^2}{4}$; podemos esperar que en promedio el algoritmo ordene la secuencia en la mitad del tiempo que toma para el peor caso.

## Referencias ##

Cormen T., Leiserson C., Rivest R. y Stein C. (2009) Introduction to Algorithms (2da ed.). MIT Press.

Golin, M. (2003) COMP271 Design and Analysis of Algorithms [notas de clase]. Disponible en: http://www.cs.ust.hk/faculty/golin/COMP271Sp03/Notes/Ins_Sort_Average_Case.pdf

Skiena S. (2010) The Algorithm Design Manual (2da ed.) Springer.
