---
title: "Capítulo 8. Programación dinámica"
author: "Diego Munguía Molina ^[Esta obra está bajo una Licencia Creative Commons Atribución 4.0 Internacional.]"
date: "Marzo, 2019"
institute: "Ingeniería en Computación, TEC"
geometry: margin=1in
header-includes:
    - \usepackage{setspace}
    - \usepackage{comment}
    - \providecommand{\subtitle}[1]{\usepackage{titling} \posttitle{\par\large#1\end{center}}}
    - \usepackage[spanish]{babel}
    - \usepackage[linesnumbered,ruled,vlined,spanish,onelanguage]{algorithm2e}
    - \usepackage{amssymb}
    - \SetKwInput{KwInput}{Entradas}
    - \SetKwInput{KwOutput}{Salidas}
    - \newcommand{\twodots}{\mathrel{{.}\,{.}}\nobreak}
    - \newcommand{\assign}{\leftarrow}
    - \DeclareMathOperator{\Fibonacci}{Fibonacci}
    - \DeclareMathOperator{\fibomemo}{fibomemo}
    - \DeclareMathOperator{\cortar}{cortar}
output:
  pdf_document:
    latex_engine: xelatex
---

## Serie de Fibonacci ##

La serie de Fibonacci es una serie infinita de números naturales que tiene la siguiente forma $0, 1, 1, 2, 3, 5, 8, 13, \dots$ . Observamos que cualquier número de la serie es igual a la suma de los números inmediatemente anteriores, es decir $3 = 2 + 1$ y $13 = 8 + 5$. Con base en esta observación podemos intuir que el siguiente número de la serie después del $13$ es entonces $21 = 13 + 8$. En términos generales podemos determinar cuál es el $n$-ésimo número de la serie a través de la siguiente función.

$$
F(n) =
\begin{cases}
      0 & n = 0 \\
      1 & n = 1 \\
      F(n - 1) + F(n - 2) & n > 1 \\
\end{cases}
$$

Podemos expresar este cálculo como un algoritmo recursivo.

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Un número natural $n$}
    \KwOutput{El número en la $n$-ésima posición de la serie de Fibonacci}
  
    \BlankLine
    \caption{Fibonacci}
    \SetAlgoVlined
    \SetKwProg{Fn}{función}{}{}

    \If{$n = 0 \lor n = 1$} {
        \Return{$n$} \;
    } \Else {
        \Return{$\Fibonacci(n - 1) + \Fibonacci(n - 2)$} \;
    }
\end{algorithm}

Este es quizás el algoritmo más ingenuo para resolver el problema, sin embargo nos permitirá entender algunos de los principios fundamentales detrás de la técnica de programación dinámica.

Al ser un algoritmo recursivo, podemos observar su árbol de llamadas para analizar su comportamiento. La figura $\ref{fig:fibonacci}$ presenta un ejemplo de árbol de llamadas para este algoritmo cuando se ejecuta con $n = 5$.

![Fibonacci para n=5 \label{fig:fibonacci}](plots/Fibonacci.png)

Este algoritmo realiza dos llamadas recursivas, generando de esta forma un árbol binario de llamadas. El algoritmo producirá un recorrido en profundidad a través de este árbol hasta encontrar los casos base para los cuáles conocemos sus valores correspondientes. A partir de este punto se construirá la secuencia de abajo hacia arriba.

### Análisis de complejidad temporal ###

Analizando el árbol de llamadas de la figura $\ref{fig:fibonacci}$ podemos establecer la siguiente relación de recurrencia[^1]

[^1]: Para relaciones de recurrencia consulte el Apéndice IV

$$
T(n) = T(n - 1) + T(n - 2) + 1
$$

Esto quiere decir que el número de llamadas (o repeticiones) para $T(n)$ es igual al número de llamadas para $T(n - 1)$ más el número de llamadas para $T(n - 2)$ más $1$ correspondiente a la llamada raíz. Sabemos que esta última constante la podemos obviar para efectos de análisis asintótico. En otras palabras, el número de llamadas corresponde al número de nodos del subárbol izquierdo más el número de nodos del subárbol derecho. Tenemos entonces que

\begin{equation}\label{eq:a}
T(n) = T(n - 1) + T(n - 2)\tag{a}
\end{equation}

Dado que la secuencia de Fibonacci está ordenada ascendentemente, sabemos que $F(n-1) > F(n-2)$, por lo tanto el número de cálculos para identificar $F(n-1)$ también es mayor —o igual, en el caso de $F(1)$ y $F(0)$— que el número necesario para $F(n-2)$. Por tanto

\begin{equation}\label{eq:b}
T(n-1) \geq T(n-2)\tag{b}
\end{equation}

Con base en esta relación, podemos identificar las cotas inferior y superior para $T(n)$.

#### Cota inferior ####

Iniciemos con la cota inferior. Si tomamos el resultado ($\ref{eq:b}$) y sumamos $T(n-2)$ a ambos lados de la inecuación obtenemos

$$
\begin{aligned}
T(n-1) + T(n-2) &\geq T(n-2) + T(n-2)
\end{aligned}
$$

Observamos que podemos reemplazar el lado izquierdo de la inecuación por $T(n)$ de acuerdo con el resultado ($\ref{eq:a}$).

$$
\begin{aligned}
T(n) &\geq T(n-2) + T(n-2) \\
\end{aligned}
$$

Simplificamos para obtener

\begin{equation}\label{eq:c}
T(n) \geq 2 T(n-2)\tag{c}
\end{equation}

Si reemplazamos $n$ por $n-2$ en el resultado ($\ref{eq:c}$) obtenemos

$$
\begin{aligned}
T(n - 2) &\geq 2 T((n - 2) - 2) \\
T(n - 2) &\geq 2 T(n - 4) \\
\end{aligned}
$$

Con base en este resultado, podemos entonces reemplazar[^2] $T(n - 2)$ por $2 T(n - 4)$ en ($\ref{eq:c}$). De manera tal que obtenemos

[^2]: Podemos verificar que al hacer este reemplazo se mantienen las relaciones de la inecuación.

$$
\begin{aligned}
T(n) &\geq 2 (2 T(n - 4)) \\
T(n) &\geq 4 T(n - 4) \\
\end{aligned}
$$

Nuevamente, podemos reemaplazar $n$ por $n-2$ en este último resultado, para obtener

$$
\begin{aligned}
T(n - 2) &\geq 4 T((n - 2) - 4) \\
T(n - 2) &\geq 4 T(n - 6) \\
\end{aligned}
$$

Y, de igual manera, podemos reemplazar $T(n - 2)$ por $4 T(n - 6)$ en ($\ref{eq:c}$).

$$
\begin{aligned}
T(n) &\geq 2 (4 T(n - 6)) \\
T(n) &\geq 8 T(n - 6) \\
\end{aligned}
$$

Podemos repetir este proceso hasta identificar un patrón

$$
\begin{aligned}
T(n) &\geq 2 T(n - 2) \\
 &\geq 4 T(n - 4) \\
 &\geq 8 T(n - 6) \\
 &\geq 16 T(n - 8) \\
 &\dots \\
 &\geq 2^i T(n - 2i) \\
\end{aligned}
$$

Notamos que en cada transformación vamos reduciendo $n$; el valor $i$ representa un número de iteraciones de esta transformación a ejecutar. Dado que nuestra función es recursiva, nos preguntamos ¿cuántas iteraciones $i$ serán necesarias para llegar al caso base $n=0$? Dado nuestro modelo, podemos reformular la pregunta de la siguiente manera

$$
\begin{aligned}
n - 2i &= 0 \\
n &= 2i \\
\frac{n}{2} &= i \\
\end{aligned}
$$

Si utilizamos este valor en nuestra inecuación generalizada obtenemos que

$$
\begin{aligned}
T(n) &\geq 2^i T(n - 2i) \\
 &\geq 2^{\frac{n}{2}} T(0) \\
 &\geq 2^{\frac{n}{2}} \Omega(1) \\
 &\geq \Omega(2^{\frac{n}{2}} \cdot 1) \\
 &\geq \Omega(2^{\frac{n}{2}}) \\
 &\therefore T(n) = \Omega(2^{\frac{n}{2}}) \\
\end{aligned}
$$

#### Cota superior ####

En cuanto a la cota superior, podemos utilizar un acercamiento muy similar. Partimos del resultado ($\ref{eq:b}$) y sumamos $T(n-1)$ a ambos lados de la inecuación.

$$
\begin{aligned}
T(n-2) &\leq T(n-1) \\
T(n-1) + T(n-2) &\leq T(n-1) + T(n-1) \\
T(n-1) + T(n-2) &\leq 2T(n-1) \\
\end{aligned}
$$

Utilizamos el resultado ($\ref{eq:a}$) para simplificar

$$
\begin{aligned}
T(n) &\leq 2T(n-1) \\
\end{aligned}
$$

Si aplicamos, de manera similar a lo realizado para la cota inferior, un proceso repetido de reemplazo de $n$ por $n-1$ para obtener un patrón generalizado que nos acerque al caso base, obtenemos entonces

$$
\begin{aligned}
T(n) &\leq 2T(n - 1) \\
 &\leq 4T(n - 2) \\
 &\leq 8T(n - 3) \\
 &\leq 16T(n - 4) \\
 &\dots \\
 &\leq 2^iT(n - i) \\
\end{aligned}
$$

Nuevamente nos preguntamos, ¿cuántas iteraciones de $i$ son necesarias para llegar al caso base $n=0$?

$$
\begin{aligned}
n-i = 0 \\
n = i
\end{aligned}
$$

Y aplicamos el resultado

$$
\begin{aligned}
T(n) &\leq 2^iT(n - i) \\
 &\leq 2^n T(0) \\
 &\leq 2^n \mathcal{O}(1) \\
 &\leq \mathcal{O}(2^n \cdot 1) \\
 &\leq \mathcal{O}(2^n) \\
 &\therefore T(n) = \mathcal{O}(2^n) \\
\end{aligned}
$$

### Análisis de complejidad espacial ###

En términos complejidad espacial, notamos que cada llamada recursiva consume únicamente la memoria necesaria para el parámetro $n$. Sin embargo, debemos recordar que esperamos aproximadamente $2^n$ llamadas. Podemos formalizar entonces que

$$
\begin{aligned}
S(n) &= 2^n \cdot \mathcal{O}(1) \\
 &= \mathcal{O}(2^n \cdot 1) \\
 &= \mathcal{O}(2^n) \\
 \therefore S(n) &= \mathcal{O}(2^n) \\
\end{aligned}
$$

### Optimizando Fibonacci ###

Justo como intuimos anteriormente, el algoritmo no es eficiente pues tiene tanto complejidad temporal como espacial de orden exponencial. Sin embargo, si retornamos a la figura $\ref{fig:fibonacci}$, podemos observar algunas características de comportamiento que podemos aprovechar para diseñar un algoritmo con mejor rendimiento.

Observamos que hay muchos cálculos que se repiten. Para una llamada inicial con $n=5$ el algoritmo repite llamadas recursivas para todos los $n \leq 3$.

$n$    # llamadas
---    ---------------
5        1
4        1
3        2
2        3
1        5
0        3

Aún más importante, observamos que la solución para $n = 5$ contiene necesariamente a la solución para $n = 4$, igualmente la solución para $n = 4$ contiene a la solución para $n = 3$, y así sucesivamente hasta llegar a $n = 0$.

Podemos mantener el comportamiento original del algoritmo, con la diferencia de que vamos a memorizar los resultados parciales que vayamos calculando, para que cuando los requiramos nuevamente no sea necesario recalcularlos, sino simplemente reutilizarlos.

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Un número natural $n$}
    \KwOutput{El número en la $n$-ésima posición de la serie de Fibonacci}
  
    \BlankLine
    \caption{Fibonacci memo arriba-abajo}
    \SetAlgoVlined
    \SetKwProg{Fn}{función}{}{}

    \Fn{$\fibomemo(n, T)$} {
        \If{$T[n] = \varnothing$} {
            $T[n] \assign \fibomemo(n-1, T) + \fibomemo(n-2, T)$ \;
        }

        \Return{T[n]}
    }

    $T \assign [\ ]$ \;
    $T[0] \assign 0$ \;
    $T[1] \assign 1$ \;

    \Return{$\fibomemo(n, T)$} \;
\end{algorithm}

Este algoritmo construye la serie parcial de arriba hacia abajo. Es decir, de manera semejante al algoritmo original, parte desde $n$ y hace un recorrido en profundidad hasta alcanzar un caso base y a partir de este calcula va calculando los valores. La diferencia radica en que los resultados parciales se almacenan en la tabla $T$, y a partir de este punto ya no es necesario recalcularlos.

![Fibonacci con memo de arriba hacia abajo para n=5 \label{fig:fibonacci_topdown}](plots/Fibonacci_topdown.png)

Podemos observar este comportamiento en la figura $\ref{fig:fibonacci_topdown}$. En este árbol tenemos dos tipos de nodos, los nodos blancos representan llamadas recursivas para calcular el valor para el $n$ pasado como argumento, mientras que los nodos grises representan consultas a la tabla $T$ de valores parciales. Si comparamos este árbol con el de la figura $\ref{fig:fibonacci}$ notamos que los nodos grises representan subárboles en el algoritmo original que ya no son recorridos.

En términos de complejidad temporal, notamos que el árbol tienen una apariencia más bien lineal. Siempre exploraremos en profundidad la rama más izquierda del árbol, y las ramas derechas de cada nodo corresponderán a consultas a la tabla de resultados parciales. Para cada $n$ tendremos exactamente dos llamadas recursivas. Concluimos entonces que

$$
T(n) = 2n = \mathcal{O}(n)
$$

En cuánto a la complejidad espacial, tomamos en cuenta una única vez el costo de la tabla $\mathcal{O}(n)$. Cada llamada tiene complejidad constante $\mathcal{O}(1)$ pues consume el espacio para el parámetro $n$ y el puntero a la tabla $T$; y esperamos $2n$ llamadas. Por lo tanto

$$
S(n) = \mathcal{O}(n) + 2n \cdot \mathcal{O}(1) = \mathcal{O}(n)
$$

Otro posible acercamiento, es desarrollar el cálculo de abajo hacia arriba. Es decir partiendo desde los casos base desarrollando la sumas hasta llegar al $n$-ésimo  elemento.

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Un número natural $n$}
    \KwOutput{El número en la $n$-ésima posición de la serie de Fibonacci}
  
    \BlankLine
    \caption{Fibonacci memo abajo-arriba}
    \SetAlgoVlined
    \SetKwProg{Fn}{función}{}{}

    $T \assign [\ ]$ \;
    $T[0] \assign 0$ \;
    $T[1] \assign 1$ \;

    \For{$i \in [2 \twodots n]$} {
        $T[i] \assign T[i-1] + T[i-2]$ \;
    }

    \Return{$T[n]$}
\end{algorithm}

Al prescindir de la doble llamada recursiva, este algoritmo reduce a la mitad el número de repeticiones, por tanto tenemos que

$$
T(n) = n - 2 = \mathcal{O}(n)
$$

En términos de complejidad espacial, el gasto de memoria más significativo que hace el algoritmo está en la tabla, entonces

$$
S(n) = \mathcal{O}(n)
$$

En ambos casos, estos nuevos algoritmos representan una mejora significativa al algoritmo original de complejidad exponencial.

## Memorización ##

Los algoritmos optimizados que logramos diseñar para el cálculo del enésimo elemento de la serie de Fibonacci aprovechan el hecho de que la resolución del problema implica la resolución de un serie de subproblemas, y adicionalmente algunos de estos subproblemas surgen más de una vez durante el proceso. Este hecho nos permitió diseñar algoritmos que memorizan los resultados parciales de los subproblemas, para reutilizarlos con complejidad temporal $\mathcal{O}(1)$ cuando sea necesario en lugar de tener que recalcularlos cada vez. 

De esta forma, la optimización por memorización[^3] constituye un intercambio de complejidad temporal por complejidad espacial. En el momento en que un cálculo recurrente se torna un problema de rendimiento, podemos considerar la posibilidad de guardar los resultados parciales —incrementando de esta forma la complejidad espacial— para minimizar el número de cálculos —disminuyendo la complejidad temporal—.

[^3]: El término en jerga técnica que se maneja en la literatura en inglés es *memoize*.

## Corte de varillas ##

Antes de profundizar más en los aspectos teóricos de la programación dinámica, consideremos la resolución de un problema de búsqueda combinatoria, prestando atención a la repetición de subproblemas y cómo aplicar la técnica de memorización para mejorar el rendimiento de la solución.

En un sitio de construcción se está utilizando como insumo varilla de acero en diferentes longitudes. La varilla llega al sitio siempre en unidades de longitud fija de 8 metros. De acuerdo con las distintas necesidades de la construcción, estas varillas deben ser cortadas en unidades de menor longitud que pueden estar entre 1m y 8m. Por simplicidad, asumiremos en este escenario que las longitudes de las varillas siempre corresponden a un número natural.

Todas las semanas el equipo que lidera el proyecto determina una tabla de valores para cada longitud de varilla de acuerdo con las necesidades de la semana, la cual se envía al equipo de cortes para que preparen el material con antelación. Con base en esta tabla para una semana específica, queremos apoyar al equipo de cortes con un algoritmo que ayude a determinar cuál es la combinación de cortes que maximiza el valor de la varilla.

Por ejemplo, si partimos de la siguiente tabla de valores $V$

longitud   1   2   3   4   5   6   7   8
--------   --  --  --  --  --  --  --  --
valor      1   5   8   9   10  17  17  20

Observamos que el valor máximo que podemos obtener para una unidad es $22$, cortando la varilla en dos piezas, una de 2m y otra de 6m.

Con el fin de simplificar nuestros análisis y observar la técnica en acción vamos a simplificar nuestra solución al problema, produciendo únicamente el valor máximo alcanzable para una varilla. Definimos entonces el problema de la siguiente manera.

(@) **Problema**. Calcular el valor máximo alcanzable cortando una varilla.  
  **Entradas**. Un número natural $n$ que representa el tamaño de la varilla a maximizar. Una secuencia $V$ donde $\forall i \in [1 \twodots n], V[i]$ representa el valor para una varilla de tamaño $i$. Por definición $V[0] = 0$.  
  **Salida**. Un número natural que representa el valor máximo alcanzable con una varilla de tamaño $n$ y la tabla $V$ dadas.

Una forma de resolver este problema es evaluando todas las posibles combinaciones de cortes, para encontrar el máximo valor.

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Un número natural $n$ que representa el tamaño de la varilla a maximizar. Una secuencia $V$ donde $\forall i \in [1 \twodots n], V[i]$ representa el valor para una varilla de tamaño $i$. Por definición $V[0] = 0$.}
    \KwOutput{Un número natural que representa el valor máximo alcanzable con una varilla de tamaño $n$ y la tabla $V$ dadas.}
  
    \BlankLine
    \caption{Corte varilla ingenuo}
    \SetAlgoVlined
    \SetKwProg{Fn}{función}{}{}

    \Fn{$\cortar(n, V)$} {
        \If{$n = 0$} {
            \Return{$0$} \;
        }
        $valor \assign -1$ \;
        \For{$i \in [1 \twodots n]$} {
            $valor \assign \max(valor, V[i] + \cortar(n - i, V))$ \;
        }

        \Return{$valor$} \;
    }
\end{algorithm}

El algoritmo analiza todas las posibles configuraciones de cortes. Nos podemos preguntar ¿cuántas configuraciones podemos esperar? La figura $\ref{fig:corte_varilla}$ presenta un modelo que podemos utilizar para calcular este número.

![Ejemplos de combinaciones de cortes para una varilla de tamaño 6 \label{fig:corte_varilla}](plots/Corte_varilla.png)

En la figura $\ref{fig:corte_varilla}$ representamos una varilla de tamaño $n = 6$ como un número binario de $n - 1$ dígitos, en este número un dígito `1` representa un punto de corte en la varilla. Así, en nuestro ejemplo, el número `00010` representa un corte que genera dos varillas, una de tamaño $4$ y otra de tamaño $2$.

Observamos que todas las posibles combinaciones de cortes que podemos hacer en una varilla de tamaño $n$ es igual a la cantidad de números binarios que podemos representar con $n - 1$ dígitos.

$$
T(n) = 2^{n-1} = \mathcal{O}(2^n)
$$

Nuestro algoritmo ingenuo tiene una complejidad temporal exponencial. ¿Podemos optimizarlo de alguna manera? La figura $\ref{fig:corte_varilla_ingenuo}$ presenta el árbol de llamadas recursivas que genera el algoritmo cuando se invoca con $n = 5$.

![Corte de varilla ingenuo para n = 5 \label{fig:corte_varilla_ingenuo}](plots/Corte_varilla_ingenuo.png)

A partir de este árbol notamos que el problema cumple con las características que permiten solucionarlo aplicando programación dinámica. Observamos superposición de problemas con una repetición de llamadas recursivas: dos subárboles con raíz $3$, cuatro subárboles con raíz $2$, ocho subárboles con raíz $1$. También, de acuerdo con la línea 6 del algoritmo y la figura $\ref{fig:corte_varilla_ingenuo}$, determinamos que la mejor solución para una varilla de tamaño $5$ incluye una selección entre las mejores soluciones para varillas de tamaño $4$, $3$, $2$ y $1$. De la misma forma, la mejor solución para una varilla de tamaño $4$ incluye una selección entre las mejores soluciones para varillas de tamaño $3$, $2$ y $1$. Sucesivamente podemos identificar el mismo patrón para las varillas de los tamaños menores que $4$. Podemos afirmar entonces que el problema tiene subestructura óptima.

Estudiamos a continuación dos posibles soluciones al problema utilizando programación dinámica. La primera utiliza un acercamiento de arriba hacia abajo, mientras que la segunda lo hace abajo hacia arriba.

Solucionamos el problema de arriba hacia abajo manteniendo el recorrido recursivo en profundidad pero aplicando *memoizing* para recordar y reutilizar los resultados previamente calculados.

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Un número natural $n$ que representa el tamaño de la varilla a maximizar. Una secuencia $V$ donde $\forall i \in [1 \twodots n], V[i]$ representa el valor para una varilla de tamaño $i$. Por definición $V[0] = 0$.}
    \KwOutput{Un número natural que representa el valor máximo alcanzable con una varilla de tamaño $n$ y la tabla $V$ dadas.}
  
    \BlankLine
    \caption{Corte varilla arriba-abajo}
    \SetAlgoVlined
    \SetKwProg{Fn}{función}{}{}

    \Fn{$\cortar(n, V, T)$} {
        \If{$T[n] \geq 0$} {
            \Return{$T[n]$} \;
        }
        $valor \assign -1$ \;
        \For{$i \in [1 \twodots n]$} {
            $valor \assign \max(valor, V[i] + \cortar(n - i, V, T))$ \;
        }

        $T[n] \assign valor$ \;
        \Return{$valor$} \;
    }

    $T \assign [-1, -1, \dots, -1]$ de tamaño $n + 1$ \;
    $T[0] \assign 0$ \;

    \Return{$cortar(n, V, T)$} \;
\end{algorithm}

Ante el requerimiento de retornar el valor maximizado para una varilla de tamaño $n$, la función primero busca el valor correspondiente en una tabla donde se van almacenando los valores ya calculados (líneas 2-3), si no lo encuentra en la tabla procede a calcularlo siguiendo el mismo recorrido recursivo que la solución ingenua estudiada anteriormente (líneas 4-6), una vez calculado lo almacena en la tabla para futura referencia (línea 7).

La figura $\ref{fig:corte_varilla_arriba_abajo}$ ilustra el comportamiento de este algoritmo. En esta figura las llamadas que calculan sus valores recursivamente (líneas 4-6) se representan como líneas sólidas, mientras que las llamadas que se resuelven encontrando valores en la tabla (líneas 2-3) se representan con líneas punteadas.

![Corte de varilla arriba-abajo para n = 5 \label{fig:corte_varilla_arriba_abajo}](plots/Corte_varilla_arriba_abajo.png)

Podemos observar que los subárboles que aparecen repetidos en la figura $\ref{fig:corte_varilla_ingenuo}$ ya no se expanden en la figura $\ref{fig:corte_varilla_arriba_abajo}$. Esto es resultado del *memoizing*, que nos permite evitar recalcular valores ya conocidos.

Con respecto a la complejidad temporal, siendo este un algoritmo recursivo, nos enfocaremos en determinar la cantidad de llamadas (o lo que es lo mismo, la cantidad de nodos en el árbol de llamadas) realizadas. La llamada recursiva (línea 6) produce la verticalidad del árbol, a través de un recorrido en profundidad por la rama más izquierda del árbol; este recorrido es de longitud $n$.

Cuando el recorrido en profundidad llega a la hoja, inicia un retorno hacia arriba. Conforme va retornando, el algoritmo continúa su recorrido horizontalmente (línea 5), los valores requeridos en este recorrido ya están almacenados en la tabla *memoized*. Observamos que para cada nivel $i$ del árbol se hacen $n$ llamadas, donde $n$ es el padre de los nodos en el nivel $i$.

Determinamos entonces que

$$
T(n) = n \cdot n = \mathcal{O}(n^2)
$$

Con respecto a la complejidad espacial, cada llamada consume $\mathcal{O}(1)$ espacios de memoria, sin embargo debemos recordar que este espacio se multiplica según la cantidad de llamadas que haga el algoritmo. Adicionalmente debemos contabilizar el espacio lineal $\mathcal{O}(n)$ que consumen respectivamente la tabla de precios y la tabla de *memoizing*. Por tanto, tenemos que

$$
\begin{aligned}
S(n) &= \mathcal{O}(1) \cdot \mathcal{O}(n^2) + \mathcal{O}(n) + \mathcal{O}(n) \\
 &= \mathcal{O}(n^2) + 2 \mathcal{O}(n) \\
 &= \mathcal{O}(n^2) + \mathcal{O}(n) \\
 &= \mathcal{O}(n^2) \\
\end{aligned}
$$

El uso de programación dinámica nos permite traer la solución de este problema a una complejidad polinomial cuadrática, mucho más tratable que la complejidad exponencial de la solución ingenua.

El siguiente algoritmo presenta otra forma de solucionar este problema con programación dinámica, esta vez siguiendo un recorrido de abajo hacia arriba. Este acercamiento parte directamente de los casos base y construye incrementalmente hasta llegar al tamaño de varilla que nos interesa.

\begin{algorithm}[H]
    \DontPrintSemicolon
    \KwInput{Un número natural $n$ que representa el tamaño de la varilla a maximizar. Una secuencia $V$ donde $\forall i \in [1 \twodots n], V[i]$ representa el valor para una varilla de tamaño $i$. Por definición $V[0] = 0$.}
    \KwOutput{Un número natural que representa el valor máximo alcanzable con una varilla de tamaño $n$ y la tabla $V$ dadas.}
  
    \BlankLine
    \caption{Corte varilla abajo-arriba}
    \SetAlgoVlined
    \SetKwProg{Fn}{función}{}{}

    $T \assign [-1, -1, \dots, -1]$ de tamaño $n + 1$ \;
    $T[0] \assign 0$ \;

    $valor \assign -1$ \;
    \For{$j \in [1 \twodots n]$} {
        \For{$i \in [1 \twodots j]$} {
            $valor \assign \max(valor, V[i] + T[j - i])$ \;
        }

        $T[j] \assign valor$ \;
    }

    \Return{$valor$} \;
\end{algorithm}

En este algoritmo, el ciclo $j$ (línea 4) permite ir calculando los valores óptimos para cada tamaño de varilla $j \leq n$, empezando por la varilla de tamaño $1$ e incrementando sucesivamente hasta llegar a la varilla de tamaño $n$, memorizando dichos valores al final de cada iteración (línea 7). Por otro lado el ciclo $i$ (línea 5) investiga todas las posibles combinaciones para una varilla de tamaño $j$; específicamente (en la línea 6) se calcula el valor de cortar la varilla de tamaño $j$ en una varilla de tamaño $i$ (`V[i]`) más el valor de la mejor manera de cortar ,la varilla de tamaño $j - i$ (`T[j-i]`). Por ejemplo para una varilla de tamaño $j = 4$ el algoritmo prueba las siguientes posibilidades y escoge la de máximo valor:

* El valor de una varilla de tamaño $1$ más el valor de la mejor manera de cortar una varilla de tamaño $3$.
* El valor de una varilla de tamaño $2$ más el valor de la mejor manera de cortar una varilla de tamaño $2$.
* El valor de una varilla de tamaño $3$ más el valor de la mejor manera de cortar una varilla de tamaño $1$.
* El valor de una varilla de tamaño $4$

Vale notar que puesto que los valores se construyen de abajo hacia arriba (o desde $1$ hasta $n$), por tanto en el momento de calcular el valor de la mejor manera de cortar una varilla de tamaño $j=4$ ya tendremos almacenados en la tabla los valores para varillas de tamaño $3$, $2$ y $1$.

Para calcular la complejidad temporal observamos que los ciclos de $j$ e $i$ (líneas 4-5) son dependientes, por tanto podemos formalizarlos de la siguiente manera

$$
\begin{aligned}
T(n) &= \sum_{j=1}^{n} \sum_{i=1}^{j} 1 \\
 &= \sum_{j=1}^{n} j \\
 &= \frac{n(n+1)}{2} \\
 &= \mathcal{O}(n^2)
\end{aligned}
$$

En cuanto a la complejidad espacial, contabilizamos las casillas de espacio consumidas por las tablas $V$ y $T$, tal que

$$
\begin{aligned}
S(n) = n + n = 2n = \mathcal{O}(n)
\end{aligned}
$$

El acercamiento de abajo hacia arriba sigue manteniendo una complejidad temporal cuadrática (en comparación con el acercamiento de arriba hacia abajo). Sin embargo, al pasar de una técnica recursiva a iterativa, se mejora el consumo de memoria reduciendo la complejidad espacial de cuadrática a lineal.

## Programación dinámica ##

La programación dinámica es una técnica que mejora la eficiencia en la resolución de problemas de búsqueda combinatoria a través de la memorización de resultados parciales o intermedios. Para poder aplicar esta técnica, nuestro problema debe cumplir con dos características particulares: debe presentar subestructura óptima, y superposición de subproblemas.

La **subestructura óptima** requiere, primero, que el problema se pueda dividir en subproblemas más pequeños, semejante al caso de la técnica de divide y conquista. Pero, a diferencia de esta última, requiere además que la solución óptima al problema contenga, o esté compuesta por, las soluciones óptimas de sus subproblemas. En el caso de Fibonacci, notamos anteriormente que la solución para $F(5)$ contiene a la solución para $F(4)$ y así sucesivamente hasta llegar a $F(0)$. Esta característica sugiere además que los problemas de programación dinámica tienen una naturaleza recursiva.

Por otro lado, la **superposición de problemas** indica que durante el proceso de solución del problema original, nos encontraremos varias veces con la necesidad de resolver un mismo subproblema. En el caso de Fibonacci, observamos que para $F(5)$ es necesario resolver más de una vez los subproblemas $F(0), F(1), F(2)$ y $F(3)$. Dada esta característica, determinamos que una forma de reducir la complejidad temporal del algoritmo, es memorizar las soluciones a los subproblemas para no tener que recalcularlos cuando aparezcan por segunda, tercera, o enésima vez durante el proceso de solución del problema original. Este caso representa una instancia de una regla heurística de optimización de algoritmos: *la optimización del costo de la complejidad temporal usualmente requiere de un mayor gasto en términos de complejidad espacial, y viceversa*.

También, como observamos en nuestros ejemplos de la serie de Fibonacci y de corte de varilla, los problemas de programación dinámica pueden ser resueltos ya sea con un acercamiento de arriba hacia abajo, a través de memorización de resultados parciales; o con un acercamiento de abajo hacia arriba reconstruyendo la solución desde sus casos base.

Para poder decidir si es pertinente aplicar esta técnica, y también para determinar cómo diseñar el algoritmo, primero es necesario hacer un análisis del problema y determinar cuál es su subestructura óptima (si es que presenta) y cuál es el patrón de repetición de subproblemas. Esto permitirá determinar cómo memorizar los valores intermedios y cómo utilizarlos en el cálculo de los valores finales.

## Complejidad temporal pseudopolinómica ##

La técnica de programación dinámica se puede utilizar en problemas difíciles como por ejemplo el problema de la mochila, la suma de subconjuntos o el vendedor viajero.

Consideremos que la complejidad temporal para la solución al problema de la suma de subconjuntos utilizando programación dinámica.

$$
\begin{aligned}
T(n) = \mathcal{O}(nk)
\end{aligned}
$$

Donde $n$ es el tamaño del conjunto de entrada, y $k$ representa el objetivo de la suma para los subconjuntos. No entraremos en detalle sobre cómo llegar a esta solución, ni cómo hacer el análisis, para esta sección sólo necesitamos tomar en cuenta el resultado del análisis.

¿Qué implicaciones trae afirmar que hay un algoritmo con complejidad temporal $\mathcal{O}(nk)$ para resolver este problema para el cual inicialmente sólo conocíamos soluciones con complejidad temporal exponencial $\mathcal{O}(2^n)$?

Para acercarnos a esta cuestión, primero tenemos que preguntarnos ¿cómo podemos clasificar $\mathcal{O}(nk)$?

Como no conocemos de antemos cuál es la relación entre $n$ y $k$ debemos considerar dos casos posibles. Si $n > k$ entonces podemos afirmar que $n^2 > nk$ y por lo tanto $\mathcal{O}(nk) = \mathcal{O}(n^2)$. Por otro lado si $k > n$ entonces $k^2 > nk$ y por tanto $\mathcal{O}(nk) = \mathcal{O}(k^2)$. 

En cualquiera de los dos casos podríamos vernos tentados a afirmar que $\mathcal{O}(nk)$ es una complejidad polinómica para el problema de suma de subconjuntos.

Esto nos plantea la posibilidad de haber encontrado una solución polinómica para un problema $\mathcal{NP}-completo$. ¿Quiere esto decir entonces que estamos a un paso de demostrar que $\mathcal{P}=\mathcal{NP}$? La respuesta es que no lo estamos ya que el resultado $\mathcal{O}(nk)$ se clasifica como **pseudopolinómico**.

Esto quiere decir que la complejidad es polinómica con respecto al valor o magnitud de la entrada, pero exponencial con respecto al tamaño de la entrada entendido como el número de bits requerido para representarla.

Analicemos un nuevo ejemplo para ilustrar mejor este concepto. Consideremos la siguiente función escrita en Python que implementa un algoritmo de fuerza bruta para determinar si un número dado es primo.

```
def es_primo(n):
    for i in range(2, n):
        if n % i == 0:
            return False
    return True
```

En primera instancia, de acuerdo con el modelo RAM, podríamos determinar que la complejidad temporal de esta función es  $T(n) = \mathcal{O}(n)$. Sin embargo, siendo más precisos tendríamos que tomar en cuenta la complejidad de calcular la función módulo (`%`), la cual sabemos que toma $\mathcal{O}(n^2)$ en Python para números muy grandes que requieren más de cuatro bytes para su representación. Con esta consideración llegaríamos a la conclusión de que $T(n) = \mathcal{O}(n^3)$. En cualquiera de los dos casos diríamos que la complejidad temporal de la función es polinómica.

Sin embargo, en su definición más precisa consideramos que la complejidad temporal es una función del tamaño de la entrada **en bits**. Esto quiere decir que si necesitamos $n_{bits}$

$$
n_{bits} = \lceil \log_2 n \rceil
$$

para representar la entrada $n$ entonces

$$
n = 2^{n_{bits}}
$$

De esta forma la función de complejidad temporal sería

$$
T(n_{bits}) = \mathcal{O}(2^{3n_{bits}})
$$

Lo cual explica la complejidad inherente que conocemos conlleva la búsqueda exhaustiva de posibles divisores para $n$ en el algoritmo de fuerza bruta para la prueba de primalidad.

Otra manera de entender este escenario es que con este algoritmo estamos recorriendo todos los antecesores de $n$, y en este punto es importante recordar que la complejidad la observamos para tamaños de entrada muy grandes. Entonces por ejemplo, para representar los números naturales en el rango $[0 \twodots 1024]$ necesitamos hasta un máximo de $2^{10}$ bits, pero para representar el natural $1025$ necesitamos $2^{11}$ bits. Cada vez que agregamos un bit más a la representación de la entrada su tamaño crece en un factor de $2$, o en otras palabras, se duplica. Con el paso de $2^{10}$ bits a $2^{11}$ bits el tamaño de la entrada crece de $1024+1$ números posibles a $2048+1$ números posibles, esto explica el crecimiento exponencial.

Tomemos en cuenta ahora el caso de la búsqueda lineal. Este algoritmo presenta complejidad temporal de $T(n) = \mathcal{O}(n)$. ¿Porqué afirmamos que este algoritmo sí es polinómico –o de forma más precisa, lineal– y no pseudopolinómico? Podemos determinar que la cantidad de bits necesaria para representar una lista crece de manera lineal y no de manera exponencial.

Por ejemplo, supongamos una lista de números enteros, y una implementación de cuatro bytes para el tipo de dato `int`. Si contabilizamos un número constante $C$ de bits para representar cada casilla –por ejemplo utilizado en punteros–, entonces una lista de $5$ elementos requeriría de $5 \cdot (C + 32)$ bits para representarla, si agregamos un elemento más entonces el tamaño de la entrada crecería linealmente en una unidad de $(C + 32)$ bits, es decir sería ahora $6 \cdot (C + 32)$. De manera general serían necesarios $n \cdot (C + 32)$ bits para representarla, esta es una función lineal sobre $n$.

Volviendo al problema de la suma de subconjuntos y su complejidad temporal $\mathcal{O}(nk)$, con programación dinámica recorremos todos los posibles subconjuntos de tamaños $0$ hasta $n$, es decir $n$ y todos sus antecesores, de manera semejante a como lo hicimos con la prueba de primalidad de fuerza bruta. Por esta razón la solución del problema sigue siendo exponencial para $n$s muy grandes. Para entender con mayor claridad la afirmación en este párrafo es recomendable resolver el ejercicio #2.

## Ejercicios ##

1. Proponga un algoritmo de programación dinámica para encontrar el $n$-ésimo elemento de la serie de Fibonacci que presente complejidad temporal $\mathcal{O}(n)$ y complejidad espacial $\mathcal{O}(1)$.

2. ¿Es posible resolver el problema de suma de subconjuntos del capítulo 7 con programación dinámica? ¿Presenta subestructura óptima? ¿Presenta superposición de problemas?

3. Debemos enviar paquetes de ayuda a una región que ha sido impactada por un desastre natural. Los paquetes serán transportados por tierra en camiones. Cada camión tiene una medida volumétrica de capacidad máxima de transporte. Los paquetes se categorizan por tipo: alimentación, higiene, medicamentos, entre otros más. Cada tipo de paquete tiene su propia medida de volumen. En diferentes momentos habrán tipos de paquetes que tengan más prioridad que otros, por ejemplo inicialmente el agua y medicamentos tendrán mayor prioridad que ropa o útiles escolares. Los volúmenes y prioridades (modeladas numéricamente) de los paquetes se registran en sendas tablas. Necesitamos diseñar un algoritmo para determinar la manera óptima de cargar un camión, maximizando ambos el espacio utilizado en el camión y la prioridad. Por ejemplo:

* Tabla de volúmenes (en $m^3$) para paquetes de Agua (A1, A2), Comida (C1), Medicamentos (M1, M2) e Higiene (H1)

A1 A2 C1 M1 M2 H1
-- -- -- -- -- --
18 18 16 8  8  12

* Tabla de prioridades para los mismos paquetes (entre más alto el número mayor la prioridad):

A1 A2 C1 M1 M2 H1
-- -- -- -- -- --
3  3  1  4  4  2

* Capacidad del camión 40$m^3$

a. Especifique este problema como un problema computacional.
b. Proponga un algoritmo ingenuo para solucionar este problema.
c. Analice la complejidad temporal de este algoritmo.
d. Identifique cuál es la subestructura óptima.
e. Identifique cuál es la superposición de problemas.
f. Proponga un algoritmo de programación dinámica para solucionar este problema.
g. Analice la complejidad temporal de la solución con programación dinámica.

4. Por convención, en el lenguaje de programación Java los identificadores compuestos por diferentes palabras se escriben como una hilera sin espacios separando con mayúsculas las palabras, por ejemplo `ServicioUsuario` o `leerMensaje`. Si quisiéramos partir estos identificadores en sus palabras componentes (por ejemplo como parte de una herramienta de documentación automática), podríamos marcar los límites de las palabras buscando mayúsculas o el fin de la hilera. Sin embargo hay un problema con el uso de acrónimos, por ejemplo el identificador `APILibros` se debería separar en las palabras `API` y `Libros`, o `ServidorTEC` y `ServidorUCR` deberían separarse en las palabras `Servidor`, `TEC` y `Servidor`, `UCR` respectivamente. Diseñe un algoritmo utilizando programación dinámica que dado un diccionario de palabras (representado como un conjunto) y un identificador, encuentre la lista de palabras más larga posible a partir del identificador, ignorando las diferencias por mayúsculas/minúsculas. Por ejemplo, dado el diccionario $\{rest, api, apis, servicio, usuario, usuarios \}$:

* El identificador `RESTAPIServicio` produce *"rest api servicio"*
* El identificador `APISUsuario` produce *"apis usuario"*
* El identificador `ServicioUsuarios` produce *"servicio usuarios"*

## Referencias ##

Cutting a Rod | DP-13. (n.d.) Disponible en: https://www.geeksforgeeks.org/cutting-a-rod-dp-13/

Cormen T., Leiserson C., Rivest R. y Stein C. (2009) Introduction to Algorithms (2da ed.). MIT Press.

Skiena S. (2010) The Algorithm Design Manual (2da ed.) Springer.

What is pseudopolynomial time? How does it differ from polynomial time? (2014) Disponible en: https://stackoverflow.com/questions/19647658
